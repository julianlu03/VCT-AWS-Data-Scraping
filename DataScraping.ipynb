{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576c131-cf69-4a9f-91fa-5a32c9a56b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data scraping relevant information from XML files\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the namespace for VALORANT Fandom Wiki XML\n",
    "ns = {'mw': 'http://www.mediawiki.org/xml/export-0.11/'}\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(\"valorant_pages.xml\")  # Replace with your file name\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes unwanted characters like [[...]], {{...}}, and HTML tags from the text.\"\"\"\n",
    "    text = re.sub(r'\\[\\[.*?\\]\\]', '', text)\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_chunks(text, max_chunk_size=300):\n",
    "    \"\"\"Splits the cleaned text into smaller chunks for embedding.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "    if current_chunk:  # Add the last chunk\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_section(text, start, end=None):\n",
    "    \"\"\"Extracts text between given section headers.\"\"\"\n",
    "    if start in text:\n",
    "        if end:\n",
    "            match = re.search(rf'{start}\\n(.*?)(?={end}|$)', text, re.DOTALL)\n",
    "        else:\n",
    "            match = re.search(rf'{start}\\n(.*?)(==|$)', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# List to hold all chunks from all sections\n",
    "all_chunks = []\n",
    "\n",
    "# Iterate through <text> tags within the namespace\n",
    "for text_element in root.findall(\".//mw:text\", ns):\n",
    "    text_content = text_element.text\n",
    "    # Check if the content exists\n",
    "    if text_content:\n",
    "        cleaned_summary = cleaned_description = cleaned_official_desc = cleaned_weapon = cleaned_pro_play = cleaned_terminology = None\n",
    "        # Extract different sections using the extract_section function\n",
    "        summary_text = extract_section(text_content, \"==Summary==\")\n",
    "        description_text = extract_section(text_content, \"==Description==\")\n",
    "        official_desc_text = extract_section(text_content, \"Official description}}\", \"==Agent Cosmetics==\")\n",
    "        weapon_text = extract_section(text_content, \"==List of Weapons==\", \"== Primary ==\")\n",
    "        pro_play_text = extract_section(text_content, \"===Pro Play===\")\n",
    "        terminology_text = extract_section(text_content, \"== Official Terminology ==\", \"==Navigation==\")\n",
    "        # Clean the extracted text\n",
    "        if summary_text:\n",
    "            cleaned_summary = clean_text(summary_text)\n",
    "            summary_chunks = create_chunks(cleaned_summary)\n",
    "            all_chunks.extend(summary_chunks)  # Add to all_chunks\n",
    "        if description_text:\n",
    "            cleaned_description = clean_text(description_text)\n",
    "            description_chunks = create_chunks(cleaned_description)\n",
    "            all_chunks.extend(description_chunks)\n",
    "        if official_desc_text:\n",
    "            cleaned_official_desc = clean_text(official_desc_text)\n",
    "            official_desc_chunks = create_chunks(cleaned_official_desc)\n",
    "            all_chunks.extend(official_desc_chunks)\n",
    "        if weapon_text:\n",
    "            cleaned_weapon = clean_text(weapon_text)\n",
    "            weapon_chunks = create_chunks(cleaned_weapon)\n",
    "            all_chunks.extend(weapon_chunks)\n",
    "        if pro_play_text:\n",
    "            cleaned_pro_play = clean_text(pro_play_text)\n",
    "            pro_play_chunks = create_chunks(cleaned_pro_play)\n",
    "            all_chunks.extend(pro_play_chunks)\n",
    "        if terminology_text:\n",
    "            cleaned_terminology = clean_text(terminology_text)\n",
    "            terminology_chunks = create_chunks(cleaned_terminology)\n",
    "            all_chunks.extend(terminology_chunks)  # Add to all_chunks\n",
    "\n",
    "# all_chunks contains all the cleaned and chunked text data from the XML file\n",
    "print(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a098bd-a58a-4b0d-a096-5308560d1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking and embedding the text data into vectors for our Knowledge Base\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "def chunk_data(parsed_data, chunk_size=200):\n",
    "    \"\"\"\n",
    "    Chunk parsed data into segments based on word count.\n",
    "    :param parsed_data: List of strings or paragraphs that have been parsed from XML.\n",
    "    :param chunk_size: Desired size of each chunk (based on word count).\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for section in parsed_data:\n",
    "        words = section.split()\n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            if len(current_chunk) >= chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []    \n",
    "    # Append any remaining words as the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "    #print(words)\n",
    "    #return chunks\n",
    "\n",
    "final_chunks = chunk_data(all_chunks)\n",
    "\n",
    "### Step 2 ###\n",
    "# Embed Chunks with Sagemaker\n",
    "\n",
    "# Hugging Face model details\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'sentence-transformers/all-MiniLM-L12-v2',  # You can replace with other text embedding models\n",
    "    'HF_TASK':'feature-extraction'\n",
    "}\n",
    "\n",
    "# Create Hugging Face Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version='py39',\n",
    "    env=hub,\n",
    "    role='arn:aws:iam::345594564754:role/HuggingFaceRole',  # Replace with your IAM role\n",
    ")\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # Choose a different instance type\n",
    "    endpoint_name = \"huggingface-embed-model-endpoint-eight\"\n",
    ")\n",
    "\n",
    "sagemaker_runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "\n",
    "def embed_chunk_hf(chunk, endpoint_name):\n",
    "    \"\"\"\n",
    "    Send a chunk of text to the Hugging Face endpoint for embedding.   \n",
    "    :param chunk: A string of text to be embedded.\n",
    "    :param endpoint_name: The name of the SageMaker endpoint serving the Hugging Face model.\n",
    "    :return: The embedding for the chunk.\n",
    "    \"\"\"\n",
    "    payload = {\"inputs\": chunk}    \n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )    \n",
    "    result = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return result\n",
    "\n",
    "\n",
    "embeddings = [embed_chunk_hf(final_chunks, \"huggingface-embed-model-endpoint-eight\") for chunk in final_chunks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
